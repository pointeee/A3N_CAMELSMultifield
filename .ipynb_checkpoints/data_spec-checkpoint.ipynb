{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a06e5df5-c543-427d-bfaa-f2a2a3540c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "990e24da-7cd5-4a05-a5f7-208edc19e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_P = np.load(\"/home/chenze/data_gpfs02/CAMELS_multifield/raw_data/Maps_P_SIMBA_LH_z=0.00.npy\")\n",
    "data_P = data_P.reshape(1000, 15, 256, 256)\n",
    "\n",
    "data_M = np.load(\"/home/chenze/data_gpfs02/CAMELS_multifield/raw_data/Maps_Mtot_SIMBA_LH_z=0.00.npy\")\n",
    "data_M = data_M.reshape(1000, 15, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0ada774-3464-4cdb-ab99-c56af2182071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum of log10(M_img) = f15.50328541\n",
      "Minimum of log10(M_img) = f9.66257858\n",
      "Maximum of log10(P_img) = 13.42540932\n",
      "Minimum of log10(P_img) = 0.74502987\n"
     ]
    }
   ],
   "source": [
    "field = 'Mtot'\n",
    "base = \"/home/chenze/data_gpfs02/CAMELS_multifield/raw_data/\"\n",
    "simcode = \"IllustrisTNG\"\n",
    "\n",
    "# these have length 1000\n",
    "sim_param = np.loadtxt(f'{base}/params_LH_{simcode}.txt', unpack=True)\n",
    "sim_param = np.repeat(sim_param, 15, axis=1).reshape(6, 15000).T\n",
    "\n",
    "# The 2D images\n",
    "# 1st dimension corresponds to parameter values,\n",
    "# 2nd dimension are different maps for a given simulation,\n",
    "# 3rd and 4th dimension are the image dimensions\n",
    "M_img = np.load(f'{base}/Maps_Mtot_{simcode}_LH_z=0.00.npy').reshape(15000, 256, 256)\n",
    "M_img = np.log10(M_img)\n",
    "print(f\"Maximum of log10(M_img) = {M_img.max():.8f}\")\n",
    "print(f\"Minimum of log10(M_img) = {M_img.min():.8f}\")\n",
    "M_img = (M_img - M_img.min()) / (M_img.max() - M_img.min())\n",
    "\n",
    "P_img = np.load(f'{base}/Maps_P_{simcode}_LH_z=0.00.npy').reshape(15000, 256, 256)\n",
    "P_img = np.log10(P_img)\n",
    "print(f\"Maximum of log10(P_img) = {P_img.max():.8f}\")\n",
    "print(f\"Minimum of log10(P_img) = {P_img.min():.8f}\")\n",
    "P_img = (P_img - P_img.min()) / (P_img.max() - P_img.min())\n",
    "\n",
    "imgs_all = np.zeros([15000, 2, 256, 256], dtype=np.float32)\n",
    "imgs_all[:,0,:,:] = M_img\n",
    "imgs_all[:,1,:,:] = P_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8aad03c-4cb7-40ae-ba2f-5efec951a264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_all.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "491c4bad-d40d-4fe9-baf4-e13d3f76314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/chenze/data_gpfs02/CAMELS_multifield/dataset/compiled_img_TNG.npy\", imgs_all)\n",
    "np.save(\"/home/chenze/data_gpfs02/CAMELS_multifield/dataset/compiled_params_TNG.npy\", sim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd6ca55-0911-456a-b5c6-eb1150b537f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(114514)\n",
    "shuffle = np.arange(15000)\n",
    "np.random.shuffle(shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78700359-4bf0-4ba6-8f20-35688e04e1e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m shuffle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m15000\u001b[39m)\n\u001b[1;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(shuffle)\n\u001b[0;32m----> 5\u001b[0m img_list \u001b[38;5;241m=\u001b[39m \u001b[43mimg_list\u001b[49m[shuffle]\n\u001b[1;32m      6\u001b[0m lab_list \u001b[38;5;241m=\u001b[39m lab_list[shuffle]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_list' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(114514)\n",
    "shuffle = np.arange(15000)\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "img_list = img_list[shuffle]\n",
    "lab_list = lab_list[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca37681-dc57-48a2-9515-fb3f0f743458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_list = np.load(\"/home/chenze/data_gpfs02/CAMELS_multifield/dataset/compiled_img_TNG.npy\")\n",
    "lab_list = np.load(\"/home/chenze/data_gpfs02/CAMELS_multifield/dataset/compiled_params_TNG.npy\")\n",
    "\n",
    "np.random.seed(114514)\n",
    "shuffle = np.arange(15000)\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "img_list = img_list[shuffle]\n",
    "lab_list = lab_list[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a02509a-6a1b-474b-a341-807a6f15de69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1226 , 0.6518 , 0.27586, 2.47942, 1.9066 , 1.90924],\n",
       "       [0.4574 , 0.899  , 0.54563, 0.68113, 1.29684, 1.32593],\n",
       "       [0.3966 , 0.6074 , 1.5305 , 0.61132, 0.98146, 1.07848],\n",
       "       ...,\n",
       "       [0.2782 , 0.9062 , 1.95884, 2.27521, 1.74957, 0.62201],\n",
       "       [0.4834 , 0.7282 , 3.22657, 3.78948, 1.09051, 0.51656],\n",
       "       [0.1074 , 0.8474 , 1.03814, 0.45946, 0.7626 , 1.70882]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca58dc9d-709a-4277-9cb6-0cf6fbc2d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenze/env/miniconda3/envs/torch_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import CosDataset, train_tfm, test_tfm\n",
    "from model import CNN_cosmo\n",
    "\n",
    "rng_seed = 114514\n",
    "training_ratio = 0.7\n",
    "valid_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cf0e6f-57ba-4bfd-b3e3-bdf8fa02d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274e1a17-64ae-4db4-bed7-d8c33139417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_list = np.load(\"/home/chenze/data_gpfs02/CAMELS_multifield/dataset/compiled_img_TNG.npy\")\n",
    "img_list = torch.tensor(img_list)\n",
    "lab_list = np.load(\"/home/chenze/data_gpfs02/CAMELS_multifield/dataset/compiled_params_TNG.npy\")\n",
    "lab_list = torch.tensor(lab_list)\n",
    "\n",
    "\n",
    "np.random.seed(114514)\n",
    "shuffle = np.arange(15000)\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "img_list = img_list[shuffle]\n",
    "lab_list = lab_list[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08fecd2-2594-4c10-82af-59e49ffb0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "    np.random.seed(rng_seed)\n",
    "    shuffle = np.arange(img_list.shape[0])\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    img_list = img_list[shuffle]\n",
    "    lab_list = lab_list[shuffle]\n",
    "    \n",
    "    len_training = int(len(img_list) * training_ratio)\n",
    "    len_valid = int(len(img_list) * training_ratio) + int(len(img_list) * valid_ratio)\n",
    "    \n",
    "    train_set = CosDataset(img_list[:len_training], lab_list[:len_training], tfm=train_tfm)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    valid_set = CosDataset(img_list[len_training:len_valid], lab_list[len_training:len_valid], tfm=test_tfm)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    test_set  = CosDataset(img_list[len_valid:], lab_list[len_valid:], tfm=test_tfm)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df581d98-f837-47a9-b7a3-6a1df21ee243",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (batch_data, batch_targ) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valid_loader):\n\u001b[1;32m      2\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m batch_data\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m     batch_targ \u001b[38;5;241m=\u001b[39m batch_targ\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/env/miniconda3/envs/torch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/env/miniconda3/envs/torch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/env/miniconda3/envs/torch_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/env/miniconda3/envs/torch_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/work/CAMELS_multifield_CNN/dataset.py:68\u001b[0m, in \u001b[0;36mCosDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_list[idx]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#im = Image.fromarray(np.float32(im), mode='L')\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im, label\n",
      "File \u001b[0;32m~/env/miniconda3/envs/torch_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/env/miniconda3/envs/torch_env/lib/python3.9/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/miniconda3/envs/torch_env/lib/python3.9/site-packages/torchvision/transforms/functional.py:142\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    140\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "        for step, (batch_data, batch_targ) in enumerate(valid_loader):\n",
    "            batch_data = batch_data.type(torch.FloatTensor).to(device)\n",
    "            batch_targ = batch_targ.type(torch.FloatTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9f87c3-fe44-4da1-a61c-9e7745fa2246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'SupportsArrayInterface'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Image'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates an image memory from an object exporting the array interface\n",
       "(using the buffer protocol)::\n",
       "\n",
       "  from PIL import Image\n",
       "  import numpy as np\n",
       "  a = np.zeros((5, 5))\n",
       "  im = Image.fromarray(a)\n",
       "\n",
       "If ``obj`` is not contiguous, then the ``tobytes`` method is called\n",
       "and :py:func:`~PIL.Image.frombuffer` is used.\n",
       "\n",
       "In the case of NumPy, be aware that Pillow modes do not always correspond\n",
       "to NumPy dtypes. Pillow modes only offer 1-bit pixels, 8-bit pixels,\n",
       "32-bit signed integer pixels, and 32-bit floating point pixels.\n",
       "\n",
       "Pillow images can also be converted to arrays::\n",
       "\n",
       "  from PIL import Image\n",
       "  import numpy as np\n",
       "  im = Image.open(\"hopper.jpg\")\n",
       "  a = np.asarray(im)\n",
       "\n",
       "When converting Pillow images to arrays however, only pixel values are\n",
       "transferred. This means that P and PA mode images will lose their palette.\n",
       "\n",
       ":param obj: Object with array interface\n",
       ":param mode: Optional mode to use when reading ``obj``. Will be determined from\n",
       "  type if ``None``.\n",
       "\n",
       "  This will not be used to convert the data after reading, but will be used to\n",
       "  change how the data is read::\n",
       "\n",
       "    from PIL import Image\n",
       "    import numpy as np\n",
       "    a = np.full((1, 1), 300)\n",
       "    im = Image.fromarray(a, mode=\"L\")\n",
       "    im.getpixel((0, 0))  # 44\n",
       "    im = Image.fromarray(a, mode=\"RGB\")\n",
       "    im.getpixel((0, 0))  # (44, 1, 0)\n",
       "\n",
       "  See: :ref:`concept-modes` for general information about modes.\n",
       ":returns: An image object.\n",
       "\n",
       ".. versionadded:: 1.1.6\n",
       "\u001b[0;31mFile:\u001b[0m      ~/env/miniconda3/envs/torch_env/lib/python3.9/site-packages/PIL/Image.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "Image.fromarray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8837fb5c-1c3b-4f15-84ad-61b8bf472358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 35, 144,  26,  35,  48,  67, 176,  47, 126, 233],\n",
       "         [195,   4, 107,  27, 248, 207,  20, 253,  46, 134],\n",
       "         [123,  94,  57,  26, 101,  70,  18, 212, 198,  87],\n",
       "         [ 30, 191,  40, 136,  37, 251, 110, 203, 230, 113],\n",
       "         [219, 111, 123,  66,  81, 108, 130, 125,  90, 161],\n",
       "         [216, 132, 201, 131,  94, 217,  50,  64, 181,  27],\n",
       "         [163,  33, 136, 138, 153,  18,  72,   9, 240, 223],\n",
       "         [114, 164, 121, 117,  17,  11, 184,  41,  41,  45],\n",
       "         [ 41,  19, 116, 134,  31, 182,  72, 223, 243, 148],\n",
       "         [182, 109, 194,  44, 209, 225, 183, 245, 217, 113]],\n",
       "\n",
       "        [[106,  57, 123,  17, 222, 228, 110,  40, 125,  58],\n",
       "         [180,  92,  96,  17, 123,  71, 189, 150,  32, 135],\n",
       "         [ 89,  73,  77, 212, 243,  51,  34, 124, 217,   4],\n",
       "         [ 91,  68, 239, 108, 214,  77,  91, 169,  79, 251],\n",
       "         [ 70,  78,  94,  74, 111,  92,  66, 185, 233, 255],\n",
       "         [164, 127, 161, 136, 229, 154, 168, 127, 188,  39],\n",
       "         [225, 230, 165,  19, 134,  59, 230, 124, 117, 207],\n",
       "         [157, 115,  62,  97,  12, 152, 201, 172, 226, 221],\n",
       "         [232, 110,  76,  75, 179,  74, 251, 208, 162, 224],\n",
       "         [198,  30, 114,  23,  75,  79, 173, 199,  33, 137]],\n",
       "\n",
       "        [[ 94, 221, 121,   6,  18,  64, 181,   9, 142, 100],\n",
       "         [132,  29,  54, 123,  73,  76, 246,  93, 251,  75],\n",
       "         [ 42,  22, 169, 200, 129, 113,  31, 129, 145,  29],\n",
       "         [ 44,  35, 163,  18, 121,  51, 167, 116,   0, 187],\n",
       "         [129, 118, 155, 250, 116,  31, 174, 232, 110,  94],\n",
       "         [175, 237, 136,  24,  70, 166, 196, 136, 102, 108],\n",
       "         [ 80,   3, 162, 250, 214, 159,  40, 253, 125, 105],\n",
       "         [219,  32,  60,  43, 238,  56, 184, 209,   5, 221],\n",
       "         [226, 127, 195, 100, 110, 225, 151,  60,  69,   1],\n",
       "         [250,   4, 117, 164,  53,  31, 207,  29, 179, 137]]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, 256, size=(3, 10, 10), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e24f35-7da1-4bc8-944e-2942af81d8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6f555-74fb-4487-a767-c1ee157911e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
